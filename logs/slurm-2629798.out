wandb: WARNING Disabling SSL verification.  Connections to this server are not verified and may be insecure!
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: levin-hertrich (visiblyintelligent) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/hertrich/VI_project/ODCE/wandb/run-20250508_192215-1zn5vm2s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run back-bone-freeze
wandb: ‚≠êÔ∏è View project at https://wandb.ai/visiblyintelligent/full-image
wandb: üöÄ View run at https://wandb.ai/visiblyintelligent/full-image/runs/1zn5vm2s
Loading model and processor from facebook/detr-resnet-50...
[2025-05-08 19:22:41,372][timm.models._builder][INFO] - Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)
[2025-05-08 19:22:41,899][timm.models._hub][INFO] - [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
[2025-05-08 19:22:42,727][timm.models._builder][INFO] - Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']
- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
DETR backbone frozen.
DETR prediction heads are trainable.
AdapterDetrModel initialized.
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/hertrich/VI_project/ODCE/main.py", line 262, in <module>
    main()
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/hertrich/VI_project/ODCE/main.py", line 96, in main
    model.load_state_dict(checkpoint['model_state_dict'])
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Detr:
	Unexpected key(s) in state_dict: "adapter.projection.weight", "adapter.projection.bias", "decoder.layers.0.self_attn.k_proj.weight", "decoder.layers.0.self_attn.k_proj.bias", "decoder.layers.0.self_attn.v_proj.weight", "decoder.layers.0.self_attn.v_proj.bias", "decoder.layers.0.self_attn.q_proj.weight", "decoder.layers.0.self_attn.q_proj.bias", "decoder.layers.0.self_attn.out_proj.weight", "decoder.layers.0.self_attn.out_proj.bias", "decoder.layers.0.self_attn_layer_norm.weight", "decoder.layers.0.self_attn_layer_norm.bias", "decoder.layers.0.encoder_attn.k_proj.weight", "decoder.layers.0.encoder_attn.k_proj.bias", "decoder.layers.0.encoder_attn.v_proj.weight", "decoder.layers.0.encoder_attn.v_proj.bias", "decoder.layers.0.encoder_attn.q_proj.weight", "decoder.layers.0.encoder_attn.q_proj.bias", "decoder.layers.0.encoder_attn.out_proj.weight", "decoder.layers.0.encoder_attn.out_proj.bias", "decoder.layers.0.encoder_attn_layer_norm.weight", "decoder.layers.0.encoder_attn_layer_norm.bias", "decoder.layers.0.fc1.weight", "decoder.layers.0.fc1.bias", "decoder.layers.0.fc2.weight", "decoder.layers.0.fc2.bias", "decoder.layers.0.final_layer_norm.weight", "decoder.layers.0.final_layer_norm.bias", "decoder.layers.1.self_attn.k_proj.weight", "decoder.layers.1.self_attn.k_proj.bias", "decoder.layers.1.self_attn.v_proj.weight", "decoder.layers.1.self_attn.v_proj.bias", "decoder.layers.1.self_attn.q_proj.weight", "decoder.layers.1.self_attn.q_proj.bias", "decoder.layers.1.self_attn.out_proj.weight", "decoder.layers.1.self_attn.out_proj.bias", "decoder.layers.1.self_attn_layer_norm.weight", "decoder.layers.1.self_attn_layer_norm.bias", "decoder.layers.1.encoder_attn.k_proj.weight", "decoder.layers.1.encoder_attn.k_proj.bias", "decoder.layers.1.encoder_attn.v_proj.weight", "decoder.layers.1.encoder_attn.v_proj.bias", "decoder.layers.1.encoder_attn.q_proj.weight", "decoder.layers.1.encoder_attn.q_proj.bias", "decoder.layers.1.encoder_attn.out_proj.weight", "decoder.layers.1.encoder_attn.out_proj.bias", "decoder.layers.1.encoder_attn_layer_norm.weight", "decoder.layers.1.encoder_attn_layer_norm.bias", "decoder.layers.1.fc1.weight", "decoder.layers.1.fc1.bias", "decoder.layers.1.fc2.weight", "decoder.layers.1.fc2.bias", "decoder.layers.1.final_layer_norm.weight", "decoder.layers.1.final_layer_norm.bias", "decoder.layers.2.self_attn.k_proj.weight", "decoder.layers.2.self_attn.k_proj.bias", "decoder.layers.2.self_attn.v_proj.weight", "decoder.layers.2.self_attn.v_proj.bias", "decoder.layers.2.self_attn.q_proj.weight", "decoder.layers.2.self_attn.q_proj.bias", "decoder.layers.2.self_attn.out_proj.weight", "decoder.layers.2.self_attn.out_proj.bias", "decoder.layers.2.self_attn_layer_norm.weight", "decoder.layers.2.self_attn_layer_norm.bias", "decoder.layers.2.encoder_attn.k_proj.weight", "decoder.layers.2.encoder_attn.k_proj.bias", "decoder.layers.2.encoder_attn.v_proj.weight", "decoder.layers.2.encoder_attn.v_proj.bias", "decoder.layers.2.encoder_attn.q_proj.weight", "decoder.layers.2.encoder_attn.q_proj.bias", "decoder.layers.2.encoder_attn.out_proj.weight", "decoder.layers.2.encoder_attn.out_proj.bias", "decoder.layers.2.encoder_attn_layer_norm.weight", "decoder.layers.2.encoder_attn_layer_norm.bias", "decoder.layers.2.fc1.weight", "decoder.layers.2.fc1.bias", "decoder.layers.2.fc2.weight", "decoder.layers.2.fc2.bias", "decoder.layers.2.final_layer_norm.weight", "decoder.layers.2.final_layer_norm.bias", "decoder.layers.3.self_attn.k_proj.weight", "decoder.layers.3.self_attn.k_proj.bias", "decoder.layers.3.self_attn.v_proj.weight", "decoder.layers.3.self_attn.v_proj.bias", "decoder.layers.3.self_attn.q_proj.weight", "decoder.layers.3.self_attn.q_proj.bias", "decoder.layers.3.self_attn.out_proj.weight", "decoder.layers.3.self_attn.out_proj.bias", "decoder.layers.3.self_attn_layer_norm.weight", "decoder.layers.3.self_attn_layer_norm.bias", "decoder.layers.3.encoder_attn.k_proj.weight", "decoder.layers.3.encoder_attn.k_proj.bias", "decoder.layers.3.encoder_attn.v_proj.weight", "decoder.layers.3.encoder_attn.v_proj.bias", "decoder.layers.3.encoder_attn.q_proj.weight", "decoder.layers.3.encoder_attn.q_proj.bias", "decoder.layers.3.encoder_attn.out_proj.weight", "decoder.layers.3.encoder_attn.out_proj.bias", "decoder.layers.3.encoder_attn_layer_norm.weight", "decoder.layers.3.encoder_attn_layer_norm.bias", "decoder.layers.3.fc1.weight", "decoder.layers.3.fc1.bias", "decoder.layers.3.fc2.weight", "decoder.layers.3.fc2.bias", "decoder.layers.3.final_layer_norm.weight", "decoder.layers.3.final_layer_norm.bias", "decoder.layers.4.self_attn.k_proj.weight", "decoder.layers.4.self_attn.k_proj.bias", "decoder.layers.4.self_attn.v_proj.weight", "decoder.layers.4.self_attn.v_proj.bias", "decoder.layers.4.self_attn.q_proj.weight", "decoder.layers.4.self_attn.q_proj.bias", "decoder.layers.4.self_attn.out_proj.weight", "decoder.layers.4.self_attn.out_proj.bias", "decoder.layers.4.self_attn_layer_norm.weight", "decoder.layers.4.self_attn_layer_norm.bias", "decoder.layers.4.encoder_attn.k_proj.weight", "decoder.layers.4.encoder_attn.k_proj.bias", "decoder.layers.4.encoder_attn.v_proj.weight", "decoder.layers.4.encoder_attn.v_proj.bias", "decoder.layers.4.encoder_attn.q_proj.weight", "decoder.layers.4.encoder_attn.q_proj.bias", "decoder.layers.4.encoder_attn.out_proj.weight", "decoder.layers.4.encoder_attn.out_proj.bias", "decoder.layers.4.encoder_attn_layer_norm.weight", "decoder.layers.4.encoder_attn_layer_norm.bias", "decoder.layers.4.fc1.weight", "decoder.layers.4.fc1.bias", "decoder.layers.4.fc2.weight", "decoder.layers.4.fc2.bias", "decoder.layers.4.final_layer_norm.weight", "decoder.layers.4.final_layer_norm.bias", "decoder.layers.5.self_attn.k_proj.weight", "decoder.layers.5.self_attn.k_proj.bias", "decoder.layers.5.self_attn.v_proj.weight", "decoder.layers.5.self_attn.v_proj.bias", "decoder.layers.5.self_attn.q_proj.weight", "decoder.layers.5.self_attn.q_proj.bias", "decoder.layers.5.self_attn.out_proj.weight", "decoder.layers.5.self_attn.out_proj.bias", "decoder.layers.5.self_attn_layer_norm.weight", "decoder.layers.5.self_attn_layer_norm.bias", "decoder.layers.5.encoder_attn.k_proj.weight", "decoder.layers.5.encoder_attn.k_proj.bias", "decoder.layers.5.encoder_attn.v_proj.weight", "decoder.layers.5.encoder_attn.v_proj.bias", "decoder.layers.5.encoder_attn.q_proj.weight", "decoder.layers.5.encoder_attn.q_proj.bias", "decoder.layers.5.encoder_attn.out_proj.weight", "decoder.layers.5.encoder_attn.out_proj.bias", "decoder.layers.5.encoder_attn_layer_norm.weight", "decoder.layers.5.encoder_attn_layer_norm.bias", "decoder.layers.5.fc1.weight", "decoder.layers.5.fc1.bias", "decoder.layers.5.fc2.weight", "decoder.layers.5.fc2.bias", "decoder.layers.5.final_layer_norm.weight", "decoder.layers.5.final_layer_norm.bias", "decoder.layernorm.weight", "decoder.layernorm.bias". 
Traceback (most recent call last):
  File "/home/hertrich/VI_project/ODCE/main.py", line 262, in <module>
    main()
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/hertrich/VI_project/ODCE/main.py", line 96, in main
    model.load_state_dict(checkpoint['model_state_dict'])
  File "/home/hertrich/miniconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Detr:
	Unexpected key(s) in state_dict: "adapter.projection.weight", "adapter.projection.bias", "decoder.layers.0.self_attn.k_proj.weight", "decoder.layers.0.self_attn.k_proj.bias", "decoder.layers.0.self_attn.v_proj.weight", "decoder.layers.0.self_attn.v_proj.bias", "decoder.layers.0.self_attn.q_proj.weight", "decoder.layers.0.self_attn.q_proj.bias", "decoder.layers.0.self_attn.out_proj.weight", "decoder.layers.0.self_attn.out_proj.bias", "decoder.layers.0.self_attn_layer_norm.weight", "decoder.layers.0.self_attn_layer_norm.bias", "decoder.layers.0.encoder_attn.k_proj.weight", "decoder.layers.0.encoder_attn.k_proj.bias", "decoder.layers.0.encoder_attn.v_proj.weight", "decoder.layers.0.encoder_attn.v_proj.bias", "decoder.layers.0.encoder_attn.q_proj.weight", "decoder.layers.0.encoder_attn.q_proj.bias", "decoder.layers.0.encoder_attn.out_proj.weight", "decoder.layers.0.encoder_attn.out_proj.bias", "decoder.layers.0.encoder_attn_layer_norm.weight", "decoder.layers.0.encoder_attn_layer_norm.bias", "decoder.layers.0.fc1.weight", "decoder.layers.0.fc1.bias", "decoder.layers.0.fc2.weight", "decoder.layers.0.fc2.bias", "decoder.layers.0.final_layer_norm.weight", "decoder.layers.0.final_layer_norm.bias", "decoder.layers.1.self_attn.k_proj.weight", "decoder.layers.1.self_attn.k_proj.bias", "decoder.layers.1.self_attn.v_proj.weight", "decoder.layers.1.self_attn.v_proj.bias", "decoder.layers.1.self_attn.q_proj.weight", "decoder.layers.1.self_attn.q_proj.bias", "decoder.layers.1.self_attn.out_proj.weight", "decoder.layers.1.self_attn.out_proj.bias", "decoder.layers.1.self_attn_layer_norm.weight", "decoder.layers.1.self_attn_layer_norm.bias", "decoder.layers.1.encoder_attn.k_proj.weight", "decoder.layers.1.encoder_attn.k_proj.bias", "decoder.layers.1.encoder_attn.v_proj.weight", "decoder.layers.1.encoder_attn.v_proj.bias", "decoder.layers.1.encoder_attn.q_proj.weight", "decoder.layers.1.encoder_attn.q_proj.bias", "decoder.layers.1.encoder_attn.out_proj.weight", "decoder.layers.1.encoder_attn.out_proj.bias", "decoder.layers.1.encoder_attn_layer_norm.weight", "decoder.layers.1.encoder_attn_layer_norm.bias", "decoder.layers.1.fc1.weight", "decoder.layers.1.fc1.bias", "decoder.layers.1.fc2.weight", "decoder.layers.1.fc2.bias", "decoder.layers.1.final_layer_norm.weight", "decoder.layers.1.final_layer_norm.bias", "decoder.layers.2.self_attn.k_proj.weight", "decoder.layers.2.self_attn.k_proj.bias", "decoder.layers.2.self_attn.v_proj.weight", "decoder.layers.2.self_attn.v_proj.bias", "decoder.layers.2.self_attn.q_proj.weight", "decoder.layers.2.self_attn.q_proj.bias", "decoder.layers.2.self_attn.out_proj.weight", "decoder.layers.2.self_attn.out_proj.bias", "decoder.layers.2.self_attn_layer_norm.weight", "decoder.layers.2.self_attn_layer_norm.bias", "decoder.layers.2.encoder_attn.k_proj.weight", "decoder.layers.2.encoder_attn.k_proj.bias", "decoder.layers.2.encoder_attn.v_proj.weight", "decoder.layers.2.encoder_attn.v_proj.bias", "decoder.layers.2.encoder_attn.q_proj.weight", "decoder.layers.2.encoder_attn.q_proj.bias", "decoder.layers.2.encoder_attn.out_proj.weight", "decoder.layers.2.encoder_attn.out_proj.bias", "decoder.layers.2.encoder_attn_layer_norm.weight", "decoder.layers.2.encoder_attn_layer_norm.bias", "decoder.layers.2.fc1.weight", "decoder.layers.2.fc1.bias", "decoder.layers.2.fc2.weight", "decoder.layers.2.fc2.bias", "decoder.layers.2.final_layer_norm.weight", "decoder.layers.2.final_layer_norm.bias", "decoder.layers.3.self_attn.k_proj.weight", "decoder.layers.3.self_attn.k_proj.bias", "decoder.layers.3.self_attn.v_proj.weight", "decoder.layers.3.self_attn.v_proj.bias", "decoder.layers.3.self_attn.q_proj.weight", "decoder.layers.3.self_attn.q_proj.bias", "decoder.layers.3.self_attn.out_proj.weight", "decoder.layers.3.self_attn.out_proj.bias", "decoder.layers.3.self_attn_layer_norm.weight", "decoder.layers.3.self_attn_layer_norm.bias", "decoder.layers.3.encoder_attn.k_proj.weight", "decoder.layers.3.encoder_attn.k_proj.bias", "decoder.layers.3.encoder_attn.v_proj.weight", "decoder.layers.3.encoder_attn.v_proj.bias", "decoder.layers.3.encoder_attn.q_proj.weight", "decoder.layers.3.encoder_attn.q_proj.bias", "decoder.layers.3.encoder_attn.out_proj.weight", "decoder.layers.3.encoder_attn.out_proj.bias", "decoder.layers.3.encoder_attn_layer_norm.weight", "decoder.layers.3.encoder_attn_layer_norm.bias", "decoder.layers.3.fc1.weight", "decoder.layers.3.fc1.bias", "decoder.layers.3.fc2.weight", "decoder.layers.3.fc2.bias", "decoder.layers.3.final_layer_norm.weight", "decoder.layers.3.final_layer_norm.bias", "decoder.layers.4.self_attn.k_proj.weight", "decoder.layers.4.self_attn.k_proj.bias", "decoder.layers.4.self_attn.v_proj.weight", "decoder.layers.4.self_attn.v_proj.bias", "decoder.layers.4.self_attn.q_proj.weight", "decoder.layers.4.self_attn.q_proj.bias", "decoder.layers.4.self_attn.out_proj.weight", "decoder.layers.4.self_attn.out_proj.bias", "decoder.layers.4.self_attn_layer_norm.weight", "decoder.layers.4.self_attn_layer_norm.bias", "decoder.layers.4.encoder_attn.k_proj.weight", "decoder.layers.4.encoder_attn.k_proj.bias", "decoder.layers.4.encoder_attn.v_proj.weight", "decoder.layers.4.encoder_attn.v_proj.bias", "decoder.layers.4.encoder_attn.q_proj.weight", "decoder.layers.4.encoder_attn.q_proj.bias", "decoder.layers.4.encoder_attn.out_proj.weight", "decoder.layers.4.encoder_attn.out_proj.bias", "decoder.layers.4.encoder_attn_layer_norm.weight", "decoder.layers.4.encoder_attn_layer_norm.bias", "decoder.layers.4.fc1.weight", "decoder.layers.4.fc1.bias", "decoder.layers.4.fc2.weight", "decoder.layers.4.fc2.bias", "decoder.layers.4.final_layer_norm.weight", "decoder.layers.4.final_layer_norm.bias", "decoder.layers.5.self_attn.k_proj.weight", "decoder.layers.5.self_attn.k_proj.bias", "decoder.layers.5.self_attn.v_proj.weight", "decoder.layers.5.self_attn.v_proj.bias", "decoder.layers.5.self_attn.q_proj.weight", "decoder.layers.5.self_attn.q_proj.bias", "decoder.layers.5.self_attn.out_proj.weight", "decoder.layers.5.self_attn.out_proj.bias", "decoder.layers.5.self_attn_layer_norm.weight", "decoder.layers.5.self_attn_layer_norm.bias", "decoder.layers.5.encoder_attn.k_proj.weight", "decoder.layers.5.encoder_attn.k_proj.bias", "decoder.layers.5.encoder_attn.v_proj.weight", "decoder.layers.5.encoder_attn.v_proj.bias", "decoder.layers.5.encoder_attn.q_proj.weight", "decoder.layers.5.encoder_attn.q_proj.bias", "decoder.layers.5.encoder_attn.out_proj.weight", "decoder.layers.5.encoder_attn.out_proj.bias", "decoder.layers.5.encoder_attn_layer_norm.weight", "decoder.layers.5.encoder_attn_layer_norm.bias", "decoder.layers.5.fc1.weight", "decoder.layers.5.fc1.bias", "decoder.layers.5.fc2.weight", "decoder.layers.5.fc2.bias", "decoder.layers.5.final_layer_norm.weight", "decoder.layers.5.final_layer_norm.bias", "decoder.layernorm.weight", "decoder.layernorm.bias". 
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mback-bone-freeze[0m at: [34mhttps://wandb.ai/visiblyintelligent/full-image/runs/1zn5vm2s[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250508_192215-1zn5vm2s/logs[0m
